# Multiple-GANs
Implementation of three GANs

## Overview
This is an implementation of the three GANs.  In general GANs simultaneously 
optimize two objectives, one for a discriminative model and a second for a generative model.
The generator is trained to generate realistic data, examplars which are both similar to and 
differ from the patterns in the discriminator.  The discriminator learns both the training 
data and the synthetic patterns generated by the generator.

The three GANs it implements are:
Wasserstein GAN with Gradient Penalties - 'Improved Training of Wasserstein GANs'
 -  https://arxiv.org/abs/1704.00028

'Wasserstein GAN'
 - https://arxiv.org/abs/1701.07875

DC GAN from Ian Goodfellow, et al - 'Generative Adversarial Networks'
 - https://arxiv.org/pdf/1406.2661
 
The primary reference repo was Shayne Obrien, https://github.com/shayneobrien/generative-models

One common design approach is to implement separate Model and Trainer classes for 
each GAN.  This however results in significant source duplication.  Therefore one
objective of this implementation is to implement three GANs with single GAN, Generator 
and Discriminator classes, minimizing source duplication.

Other purposes of this implementation are:
 - Evaluate multiple Generator and Discriminator networks
 - Evaluate use of the AdamW optimizer
 - As with the VAE, train using no image transformation and two different grayscale transforms

Source changes are required to enable each option.

The following are not implemented:
 - Eval mode
 - Checkpoint model archive and retrieval

## Input dataset
The version of MNIST downloaded with the TorchVision Dataset object.

## Performance
The loss for each GAN is similar to the original repo, and sample image quality is also similar.

## Python version and Conda environment
This has been tested with Python 3.7.4 on Win 10.  Use of a virtual environment is recommended.
Following is a Conda implementation:

```
conda create --name gan_env python==3.7.4 pip
conda activate
pip install -r requirements.txt
```

The requirements.txt file was generated with pipreqs, not the environment
configuration from the reference repo, but should be correct.

## Usage
All execution parameters are implemented in parse_args in utils.py.  Following is a single-epoch sample train
execution of each GAN:

```
python main_v1.py --mod_name W_GP_GAN --init_gen 0  --user_epochs 1  --gen_lr 1e-4 --disc_lr 1e-4  --init_gen 0 --batch_size 100  --weight_decay 1e-5 --disc_step 5 --data_folder ./data --seed 3435 1>ExecOut\stdOutWGANGPv1.txt  2>ExecOut\stdErrWGANGPv1.txt
python main_v1.py --mod_name W_GAN --init_gen 0  --user_epochs 1  --gen_lr 5e-5 --disc_lr 5e-5  --init_gen 0 --batch_size 100  --weight_decay 1e-5 --disc_step 5 --data_folder ./data --seed 3435 1>ExecOut\stdOutWGANv1.txt  2>ExecOut\stdErrWGANv1.txt
python main_v1.py --mod_name MM_GAN --init_gen 5  --user_epochs 1  --gen_lr 2e-4 --disc_lr 2e-4  --init_gen 0 --batch_size 100  --weight_decay 1e-5 --disc_step 1 --data_folder ./data --seed 3435 1>ExecOut\stdOutMMGANv1.txt  2>ExecOut\stdErrMMGANv1.txt
```
Also included in Exec1.bat.

## Output
Outputs
 - Generator and discriminator losses 
 - Sample images generated by the Generator during training in ./valid/[mod_name]/

## Results
Using Generator_v1 and Discriminator_v1, using the parameters defined earlier for each model, the GANs achieve losses similar to the original repo 
listed in the References section below.

## References
1. 'Improved Training of Wasserstein GANs' -  https://arxiv.org/abs/1704.00028
2. 'Wasserstein GAN' - https://arxiv.org/abs/1701.07875
3. 'Generative Adversarial Networks' - https://arxiv.org/pdf/1406.2661

The primary reference repo was Shayne Obrien, generative-model folder, [here](https://github.com/shayneobrien/generative-models),
which contains multiple well-written GANs and two VAEs.

